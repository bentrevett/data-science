{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will consolidate: model evaluation, data preprocessing, feature\n",
    "#engineering and tackling overfitting in a seven step workflow\n",
    "\n",
    "#there are four types of machine learning\n",
    "\n",
    "#supervised learning\n",
    "# learning to map inputs to targets, given a set of examples\n",
    "# most common type by far\n",
    "# mostly classification and regression but also: sequence\n",
    "# generation, syntax tree prediction, object detection, image\n",
    "# segmentation\n",
    "\n",
    "#unsupervised learning\n",
    "# finding transformations of data without any targets for\n",
    "# data viz, compression, denoising, etc.\n",
    "# mostly dimensionality reduction (tSNE) and clustering (k-means)\n",
    "\n",
    "#self-supervised learning\n",
    "# like supervised learning but without \"human labels\", there\n",
    "# are still labels involved but they are generated from the\n",
    "# data\n",
    "# mostly encoders, where the targets are the inputs; trying\n",
    "# to predict the next frame in a video, given past frames; \n",
    "# or next word in a text, given previous words\n",
    "\n",
    "# the line between the 3 supervised learning types isn't \n",
    "# set in stone, but the majority of this book is supervised\n",
    "# with some self-supervised at the end\n",
    "\n",
    "#reinforcement learning\n",
    "# agent receives information about its environment, learns to\n",
    "# choose actions that maximize a reward\n",
    "# currently limited to games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification/regression glossary\n",
    "\n",
    "#sample/input - what goes into the model\n",
    "#prediction/output - what comes out of the model\n",
    "#target - what your model should have predicted\n",
    "#pred. error/loss - distance between prediction and target\n",
    "#class - set of labels to choose from\n",
    "#label - a specific instance of a class annotation\n",
    "#ground truth/annotations - ALL targets\n",
    "#binary classification - classification with 2 classes\n",
    "#multi-class classification - classification with >2 classes\n",
    "#multi-label classification - classification where each sample\n",
    "#                             can be from >1 class\n",
    "#scalar regression - target is a continuous scalar value\n",
    "#vector regression - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split models into train, val and test\n",
    "\n",
    "#you always overfit on the train if you don't use regularization\n",
    "#want to generalize, that is achieve good accuracy on unseen\n",
    "#examples, overfitting works against this\n",
    "\n",
    "#why even use a validation set?\n",
    "#validation is used for tweaking hyperparameters\n",
    "#eventually you'll overfit on the validation set even though\n",
    "#your model has never seen it before as your hyperparameters\n",
    "#will be tuned to the validation set and you'll get poor\n",
    "#accuracy on the test set, so the test set is to check how\n",
    "#badly we are overfitting on the validation set\n",
    "\n",
    "#a concept called \"information leak\", whenever you tune \n",
    "#hyperparamters based on validation set performance, info\n",
    "#about the validation set leaks into the model, adding\n",
    "#bits of information into the model\n",
    "\n",
    "#should have no data from the test set at all, even \n",
    "#indirectly\n",
    "\n",
    "#two ways of splitting data:\n",
    "\n",
    "#1. held-out validation\n",
    "#simply split the data like 70:15:15 into train:val:test\n",
    "#if you don't have a lot of data then your validation set\n",
    "#will be small and your validation set will be a poor\n",
    "#representation of your testing set due to high variance\n",
    "\n",
    "#2. k-fold validation\n",
    "#split data into K partitions of equal size, for each part\n",
    "#train on K-1 partition and evaluate on the remaining one\n",
    "#final validation score is the averaged of the K scores\n",
    "\n",
    "#3. iteratied k-fold validation w/ shuffling\n",
    "#apply k-fold validation multiple times, shuffling the data\n",
    "#before each split into K and then getting a validation score\n",
    "#final validation score is average again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other points:\n",
    "\n",
    "#1. data representations - if you have 10 classes in your data\n",
    "# then you want each of the 10 classes equally represented\n",
    "# in each of your splits\n",
    "#2. \"arrow of time\" - if you're trying to predict the future\n",
    "# from the past, you shouldn't randomly shuffle data as you'll\n",
    "# have information from the future\n",
    "#3. redundancy - in real world data you might have some data\n",
    "# points appear multiple times, don't want same data in train\n",
    "# and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "\n",
    "#1. vectorization\n",
    "\n",
    "#data must be tensors of floats or integers\n",
    "\n",
    "#2. normalization\n",
    "\n",
    "#don't want features with different ranges or very wide ranges\n",
    "#as this can create some large parameter updates that slow down\n",
    "#convergence, ideally want all features to have their mean\n",
    "#of 0 and a std of 1\n",
    "\n",
    "#3. missing values\n",
    "\n",
    "#it's safe to use a number, such as 0 to represent a missing\n",
    "#value and the network will learn to ignore it, (only if 0 isn't\n",
    "#being used to represent something in the data) however if you \n",
    "#have missing values in the test data but not in training, the\n",
    "#network won't know how to handle this, therefore you should\n",
    "#create artifical examples by randomly dropping the features\n",
    "#that are missing in the test data within the training data\n",
    "\n",
    "#4. feature engineering\n",
    "\n",
    "#using your knowledge about the data to make the neural network\n",
    "#work better by using your own transformations on the data\n",
    "#apparently it isn't reasonable to expect the model to learn\n",
    "#from every single representation, imagine trying to get the\n",
    "#time from a picture of a clock. from the raw image it's very\n",
    "#difficult, a better way to write something to output the \n",
    "#x and y co-ordinates of the clock hands, an even better way\n",
    "#would be to express the angle of each hand (wouldn't even\n",
    "#need to use machine learning in this case)\n",
    "\n",
    "#deep learning can learn from raw data, does this mean we don't\n",
    "#need to use feature engineering? no.\n",
    "#1. good features allow you to solve the problems easier\n",
    "#   i.e. less resources, simplier network\n",
    "#2. good features allow you to use less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#over/underfitting\n",
    "\n",
    "#overfitting always happens as long as your model isn't too\n",
    "#small\n",
    "#it arises in the trade-off between optimization and\n",
    "#generalization. optimization is adjusting the parameters of\n",
    "#the model to get the best performance possible on the training\n",
    "#data, whereas generalization refers to how well the model\n",
    "#performs on data it has never seen before. \n",
    "#you want to get the best generalization as possible, but you\n",
    "#can only optimize on the training data you have\n",
    "\n",
    "#at the beginning of training, optimization and generalization\n",
    "#losses both decrease. this is your model underfitting, it has\n",
    "#yet to model the patterns in the training data. after a \n",
    "#number of iterations, generalization stops improving\n",
    "#and the validation metrices degrade, this is when the model \n",
    "#is said to be overfitting, i.e. it is learning patterns\n",
    "#specific to just the training data only.\n",
    "\n",
    "#the best solution to overfitting? get more training data.\n",
    "#with more data the model will generalize better\n",
    "\n",
    "#usually getting more data isn't available, the next best thing\n",
    "#is to reduce the information your model can store or add\n",
    "#constraints on what it can store. the model will only be \n",
    "#able to remember a reduced number of patterns and therefore\n",
    "#optimization will cause it to remember the most useful \n",
    "#patterns, which gives it a better chance of generalizing\n",
    "\n",
    "#this is called regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization\n",
    "\n",
    "#the easiest way to perform overfitting is to reduce the \n",
    "#size of the model which reduces the number of learnable\n",
    "#parameters and thus the number of patterns the model can\n",
    "#memorize (aka a compressed representation), forcing it to \n",
    "#learn the most useful ones that generalize the best\n",
    "\n",
    "#a model's number of learnable parameters is often called\n",
    "#it's capacity\n",
    "\n",
    "#however, if you reduce the parameters too much, the model\n",
    "#will underfit\n",
    "\n",
    "#there's no mathematical formula to say how many layers/\n",
    "#parameters you need, usually start with a small one, see\n",
    "#if you can overfit and if you do then increase the \n",
    "#number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight regularization\n",
    "\n",
    "#occam's razor: given two explanations for something the most\n",
    "#likely one to be correct is the simplest one as it makes the\n",
    "#least amount of assumptions\n",
    "#same can apply to neural networks, given multiple models\n",
    "#the \"simpler\" ones are less likely to overfit than complex \n",
    "#ones\n",
    "\n",
    "#a simpler model in this context means the distribution of \n",
    "#parameters has less entropy, thus a common way to prevent\n",
    "#overfitting is to put constraints on the complexity of a \n",
    "#network by forcing its weight to take only small values which\n",
    "#makes the values of the weights more regular. this is called\n",
    "#weight regulatization.\n",
    "\n",
    "#done by adding to the loss function a cost associated with\n",
    "#having large weights, there are two types:\n",
    "#1. L1 regularization - cost is the absolute value\n",
    "#                       of the parameter values (L1 norm)\n",
    "#2. L2 regulatization - cost is the squared value of the \n",
    "#                       parameter values (l2 norm)\n",
    "#L2 regularization is also known as weight decay\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, \n",
    "                       kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu',\n",
    "                       input_shape=(10_000,)))\n",
    "model.add(layers.Dense(16, \n",
    "                       kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1,\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "#this means that every value in the weight matrix will add\n",
    "#0.001 * value ** 2 to the final loss\n",
    "\n",
    "#note: this loss is only applied during training, so the \n",
    "#test loss will be a lot higher\n",
    "\n",
    "#can also do:\n",
    "# regularizers.l1(x)\n",
    "# regularizers.l1_l2(l1=x, l2=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout\n",
    "\n",
    "#most effective and commonly used regularization techniques\n",
    "#a dropout layer consists of randomly dropping (setting to zero)\n",
    "#a number of output features of a layer during training\n",
    "#i.e. if the layer would normally return [.2, .3, .4, .5]\n",
    "#after we apply dropout the layer might be [0, .3, 0, .5]\n",
    "\n",
    "#the dropout rate is the probability for each feature that\n",
    "#it will be zeroed out, usually around 0.5\n",
    "\n",
    "#during test time, no features are zeroed out, instead the\n",
    "#layer's outputs are scaled down by a factor equal to the \n",
    "#dropout rate to balance the fact that more units are\n",
    "#active than at training time, i.e. if our dropout rate was\n",
    "#0.5, then at testing time we would multiply each output\n",
    "#value of that layer by 0.5\n",
    "#we can do the scaling at training time by dividing the\n",
    "#output of the values by the dropout rate, i.e. multiply\n",
    "#by 1/0.5 = multiply by 2\n",
    "\n",
    "#the core idea of dropout is that introducing noise in the\n",
    "#network can break up coincidental pattrns that aren't \n",
    "#significant, which the network will memorize if no noise is\n",
    "#present, wasting capacity on them\n",
    "\n",
    "#use dropout in keras by adding a dropout layer\n",
    "\n",
    "model = models.Sequential() \n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \n",
    "model.add(layers.Dropout(0.5)) \n",
    "model.add(layers.Dense(16, activation='relu')) \n",
    "model.add(layers.Dropout(0.5)) \n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#YOU NEVER APPLY DROPOUT TO THE OUTPUT LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to recap, best way to prevent overfitting:\n",
    "#1. get more training data\n",
    "#2. reduce capacity of the network\n",
    "#3. add weight regularization\n",
    "#4. add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO 4.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
