{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI/ML/DL**\n",
    "\n",
    "Deep learning is a sub-field of machine learning\n",
    "Machine learning is a sub-field of AI\n",
    "\n",
    "AI is *the effort to automate intellectual tasks normally performed by humans*. AI doesn't have to have any learning, i.e. hard-coded rules, expert systems. This is called *symbolic AI*, it's good at well-defined logical problems, but not for fuzzy ones.\n",
    "\n",
    "Classical programming takes rules and data to create answers. Machine learning takes answers and data to create rules.\n",
    "\n",
    "Machine learning learns meaningful transformations of data, that is, useful representations of the input data. A representation is a different way to look at data. *This is done by doing linear transforms of the data manifold so points from different classes can be easily separated*.\n",
    "\n",
    "Learning can be thought of as automatically searching for better representations. Machine learning cannot find all representations, only those that exist in the hypothesis space.\n",
    "\n",
    "DL focuses on multiple layers of representations. Could also be known as: layered reprsentation learning or hierarchical reprsentation learning.\n",
    "\n",
    "Neural networks are **inspired** by how the brain works. There is no actual link between them and how the brain actually works. The brain doesn't do backpropagation.\n",
    "\n",
    "Each layer is an information-distillation operation, maximizing information relevant to the task and discarding irrelevant information.\n",
    "\n",
    "Each layer has *parameters* known as *weights* and *biases*. Learning counts as finding the values of the parameters which best maximize the performance at the desired task. \n",
    "\n",
    "Performance is measured with a *loss function* or *objective function*, which takes in the target output and the network's predicted output and outputs how *far* the prediction is from the target, known as *loss* or *cost*.\n",
    "\n",
    "The parameters are altering them in the direction that will lower the *loss* for the current example. This is done by an *optimizer* which implements the *backpropagation* algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRE-DL**\n",
    "\n",
    "*Probabilistic modeling* using Naive Bayes. Assumes all features are independent. Uses Bayes theorem. Another probabilistic modeling algorithm is *logistic regression*. \n",
    "\n",
    "*Kernel methods* such as *SVM*. SVM aims to find good decision boundaries between classes. It does this by first mapping the data to a higher-dimensional representation where the decision boundary can be represented as a hyperplane (with $N$ dimensional datam hyperplane is in $N-1$ dimensions), and then maximizing the distance between the hyperplane and the closest data point from each class. Mapping data into higher-dimension needs a good transformation, done with the *kernel trick*. This uses a *kernel function* which maps any two points in your initial space to the distance between these points in the target representation space. Kernel functions are usually crafted by hand, rather than learned, in an SVM only the separation hyperplane is learned. SVMs don't scale to large datasets and not good for perceptual problems such as image recognition. As SVMs are *shallow*, for perceptual problems it first needs the features to be extracted, which requires featuring engineering, which is difficult.\n",
    "\n",
    "*Decision trees* are flowchart-like structures. They are learned from data and are easy to interpret and visualize. *Random forests* are a decision tree algorithm that involves building a large number of specialized decision trees and ensembling their outputs. *Gradient boosting machines* works by ensembling weak prediction models, like decision trees. It uses gradient boosting which is a way to improve a model by iteratively training new models that address the weak points of the previous models. \n",
    "\n",
    "Neural networks and deep learning in particular has become popular because it automates the feature engineering step. **Why can't you just stack shallow models on top of each other to create a deep model? Because the optimal first representation layer in a three-layer model isn't the same as the optimal first layer in a one-layer model.** Deep learning learns all layers of representations jointly, rather than in succession. When a model changes it's internal features, all other features that depend on it automatically change. This allows for more complex, abstract representations to be learned by breaking them down into a series of intermediate representations. \n",
    "\n",
    "The two characteristics of DL are:\n",
    "- the incremental, layer-by-layer way in which increasingly complex representations are developed\n",
    "- the intermediate incremental representations are learned jointly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY NOW?**\n",
    "\n",
    "As machine learning is experimental more than theoretical, advances are only possible with the available data and hardware.\n",
    "\n",
    "Hardware advances from GPUs. Soon will be replaced by specialized hardware such as TPUs.\n",
    "\n",
    "The rise of cheap storage and the internet means it has been easier to collect large datasets. \n",
    "\n",
    "There has also been algorithmic advances. Such as: activation functions (ReLU), weight-initialization schemes (Xavier/Glorot), optimization schemes (RMSProp and Adam), batch normalization, residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
